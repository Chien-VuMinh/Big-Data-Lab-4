{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf541b9",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60abf3",
   "metadata": {},
   "source": [
    "- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123350a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, to_json, struct, when, current_timestamp, unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c92585",
   "metadata": {},
   "source": [
    "- Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Kafka configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"\n",
    "KAFKA_TOPIC_INPUT = \"btc-price\"\n",
    "KAFKA_TOPIC_HIGHER = \"btc-price-higher\"\n",
    "KAFKA_TOPIC_LOWER = \"btc-price-lower\"\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize Spark session\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"BTCPriceTransform\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        logger.info(\"Spark session created successfully\")\n",
    "\n",
    "        # Define schema for input data from btc-price topic\n",
    "        schema = StructType([\n",
    "            StructField(\"symbol\", StringType(), False),\n",
    "            StructField(\"price\", DoubleType(), False),\n",
    "            StructField(\"timestamp\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        # Read from Kafka topic btc-price\n",
    "        df = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "            .option(\"subscribe\", KAFKA_TOPIC_INPUT) \\\n",
    "            .option(\"startingOffsets\", \"latest\") \\\n",
    "            .load()\n",
    "\n",
    "        # Parse JSON data\n",
    "        df = df.select(\n",
    "            from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    "        ).select(\"data.*\")\n",
    "\n",
    "        # Convert timestamp to Spark timestamp type\n",
    "        df = df.withColumn(\"event_time\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "        # Define a 20-second window for processing\n",
    "        windowed_df = df.withWatermark(\"event_time\", \"20 seconds\") \\\n",
    "            .groupBy(\n",
    "                col(\"symbol\"),\n",
    "                col(\"price\").alias(\"base_price\"),\n",
    "                col(\"event_time\").alias(\"base_time\")\n",
    "            )\n",
    "\n",
    "        # Use Spark SQL to find higher and lower price records within 20 seconds\n",
    "        df.createOrReplaceTempView(\"price_data\")\n",
    "\n",
    "        # Query to find the first higher price within 20 seconds\n",
    "        higher_query = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                p1.symbol,\n",
    "                p1.base_price,\n",
    "                p1.base_time,\n",
    "                MIN(p2.event_time) as higher_time\n",
    "            FROM price_data p1\n",
    "            LEFT JOIN price_data p2\n",
    "            ON p2.event_time > p1.base_time\n",
    "            AND p2.event_time <= p1.base_time + INTERVAL 20 SECONDS\n",
    "            AND p2.price > p1.base_price\n",
    "            GROUP BY p1.symbol, p1.base_price, p1.base_time\n",
    "        \"\"\")\n",
    "\n",
    "        # Query to find the first lower price within 20 seconds\n",
    "        lower_query = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                p1.symbol,\n",
    "                p1.base_price,\n",
    "                p1.base_time,\n",
    "                MIN(p2.event_time) as lower_time\n",
    "            FROM price_data p1\n",
    "            LEFT JOIN price_data p2\n",
    "            ON p2.event_time > p1.base_time\n",
    "            AND p2.event_time <= p1.base_time + INTERVAL 20 SECONDS\n",
    "            AND p2.price < p1.base_price\n",
    "            GROUP BY p1.symbol, p1.base_price, p1.base_time\n",
    "        \"\"\")\n",
    "\n",
    "        # Calculate time differences in seconds\n",
    "        higher_df = higher_query.withColumn(\n",
    "            \"higher_window\",\n",
    "            when(col(\"higher_time\").isNotNull(),\n",
    "                 (unix_timestamp(col(\"higher_time\")) - unix_timestamp(col(\"base_time\"))).cast(\"double\")\n",
    "            ).otherwise(20.0)\n",
    "        ).select(\n",
    "            col(\"base_time\").cast(\"string\").alias(\"timestamp\"),\n",
    "            col(\"higher_window\")\n",
    "        )\n",
    "\n",
    "        lower_df = lower_query.withColumn(\n",
    "            \"lower_window\",\n",
    "            when(col(\"lower_time\").isNotNull(),\n",
    "                 (unix_timestamp(col(\"lower_time\")) - unix_timestamp(col(\"base_time\"))).cast(\"double\")\n",
    "            ).otherwise(20.0)\n",
    "        ).select(\n",
    "            col(\"base_time\").cast(\"string\").alias(\"timestamp\"),\n",
    "            col(\"lower_window\")\n",
    "        )\n",
    "\n",
    "        # Convert to JSON for Kafka output\n",
    "        higher_output = higher_df.select(\n",
    "            to_json(struct(\"timestamp\", \"higher_window\")).alias(\"value\")\n",
    "        )\n",
    "\n",
    "        lower_output = lower_df.select(\n",
    "            to_json(struct(\"timestamp\", \"lower_window\")).alias(\"value\")\n",
    "        )\n",
    "\n",
    "        # Write to Kafka topic btc-price-higher\n",
    "        higher_query = higher_output.writeStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "            .option(\"topic\", KAFKA_TOPIC_HIGHER) \\\n",
    "            .option(\"checkpointLocation\", \"/tmp/spark/checkpoint/higher\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start()\n",
    "\n",
    "        # Write to Kafka topic btc-price-lower\n",
    "        lower_query = lower_output.writeStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "            .option(\"topic\", KAFKA_TOPIC_LOWER) \\\n",
    "            .option(\"checkpointLocation\", \"/tmp/spark/checkpoint/lower\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start()\n",
    "\n",
    "        logger.info(\"Started streaming to Kafka topics btc-price-higher and btc-price-lower\")\n",
    "\n",
    "        # Await termination\n",
    "        spark.streams.awaitAnyTermination()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Streaming failed: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        logger.info(\"Closing Spark session\")\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d461e08",
   "metadata": {},
   "source": [
    "- Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafafdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
