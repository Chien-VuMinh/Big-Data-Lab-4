{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, explode, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "# from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"KafkaToMongoDB\"\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"\n",
    "KAFKA_TOPIC = \"btc-price-zscore\"\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "MONGO_DB_NAME = \"btc_data\"\n",
    "SPARK_KAFKA_PACKAGE = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.jars.packages\", SPARK_KAFKA_PACKAGE) \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"zscore_data\", ArrayType(StructType([\n",
    "        StructField(\"window\", StringType(), True),\n",
    "        StructField(\"zscore_price\", FloatType(), True)\n",
    "    ])), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the JSON data from Kafka\n",
    "json_df = kafka_df.selectExpr(\"CAST(value AS STRING) as value\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# explode the zscore_data array and convert timestamp to a proper format\n",
    "expanded_df = json_df.withColumn(\"zscore\", explode(\"zscore_data\")) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(\"timestamp\")) \\\n",
    "    .select(\n",
    "        col(\"timestamp\"),\n",
    "        col(\"symbol\"),\n",
    "        col(\"zscore.window\").alias(\"window\"),\n",
    "        col(\"zscore.zscore_price\").alias(\"zscore_price\"),\n",
    "        col(\"event_time\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling late data (10 second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermarked_df = expanded_df.withWatermark(\"event_time\", \"10 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_mongo(df, epoch_id):\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[MONGO_DB_NAME]\n",
    "    for row in df.collect():\n",
    "        collection_name = f\"{KAFKA_TOPIC}-{row['window']}\"\n",
    "        document = {\n",
    "            \"timestamp\": row[\"timestamp\"],\n",
    "            \"symbol\": row[\"symbol\"],\n",
    "            \"window\": row[\"window\"],\n",
    "            \"zscore_price\": row[\"zscore_price\"]\n",
    "        }\n",
    "        db[collection_name].insert_one(document)\n",
    "        print(f\"[Epoch {epoch_id}] Ghi v√†o {collection_name}: {document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = watermarked_df.writeStream \\\n",
    "    .foreachBatch(write_to_mongo) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
